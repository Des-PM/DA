# -*- coding: utf-8 -*-
"""Randomforest/GradientBoost/SupportVector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r9I-ADIEyzmgepQB3XWzHEcAHyugr1KG
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE  # imblearn library can be installed using pip install imblearn
from sklearn.preprocessing import StandardScaler
from imblearn.pipeline import Pipeline
from sklearn import metrics
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn.datasets as dataz
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as MSE
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Importing dataset and examining it
df = pd.read_csv("/content/telecom_customer_churn.csv")
pd.set_option('display.max_columns',None) #Will ensure that all columns are displayed

#Display the top 5 rows
df.head()
#Display the bottom 5 rows
#dataset.tail()

df.shape
df.describe() # Quick Description of the data

df.describe(include='all') # will include all variables

df=df.drop(columns=['Zip Code','Latitude','Longitude','Customer ID','Churn Category','Churn Category','City'])

df.info()  # general information about the dataset, including data type and missing values.

#Using Seaborn library 
sns.pairplot(df)

sns.heatmap(df.corr(), annot= True)

# Increase the size of the heatmap.
plt.figure(figsize=(16, 6))
# Store heatmap object in a variable to easily access it when you want to include more features (such as title).
# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.
heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)
# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);

plt.figure(figsize=(16, 6))
heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);
# save heatmap as .png file
# dpi - sets the resolution of the saved image in dots/inches
# bbox_inches - when set to 'tight' - does not allow the labels to be cropped
plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')

#Heatmap with cluster information
sns.clustermap(df.corr(), method="complete",cmap='RdBu',annot=True,annot_kws={"size":10}, vmin=1,figsize=(10,7))

"""In this dataset, 'Total Revenue' is the target variable. dabl helps to understand how the distribution of the total revenue looks like and also what kind of relationship different features have with total revenue. The relationship of continuous features with the target is seen in scatter plot. From the first scatter plot it's clear that total charges have strong relationship with total revenue similarly tenure in months also has a strong relation with total revenue whereas total refunds has a weak relation with total revenue. The categorical features of the dataset are visualized using a box plot where the data is distributed based on the median and the quartile range and it also shows the outliers."""

#use dabl to explore the relationships
!pip install dabl
import dabl
dabl.plot(df, 'Total Revenue')

df = pd.get_dummies(df, prefix = None)
df.head()

df.info()

df['Avg Monthly Long Distance Charges'].fillna(df['Avg Monthly Long Distance Charges'].mean(),inplace = True)
df['Avg Monthly GB Download'].fillna(df['Avg Monthly GB Download'].mean(),inplace = True)

X=df.drop(columns=['Total Revenue'])
Y=df.loc[:,'Total Revenue']

X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size =0.3, random_state = 7)

# Print to check out the size of the train and test datasets
print(f'X_train: {X_train.shape}')
print(f'X_test:  {X_test.shape}')
print(f'y_train: {Y_train.shape}')
print(f'y_test:  {Y_test.shape}')

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.values)
X_test_scaled = scaler.fit_transform(X_test.values)

"""##Random forest.

Random Forest Regression uses ensemble learning method for regression. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.A Random Forest operates by constructing several decision trees during training time and outputting the mean of the classes as the prediction of all the trees.
"""

# Random Forest Regressor"
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as mse

# Implementing Random Forest Regression
# Tuning the random forest parameter 'n_estimators' and implementing cross-validation using Grid Search
rfm = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
grid_param = {'n_estimators': [10,20,30,50,100,150,200,250,300]}
gd_sr = GridSearchCV(estimator=rfm, param_grid=grid_param, scoring='r2', cv=5)
gd_sr.fit(X_train, Y_train)
best_parameters = gd_sr.best_params_
print("Optimal parameters:", best_parameters)
best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\n",
print("Best mean cross-validated score:", best_result)

rfm.fit(X_train,Y_train)
featimp = pd.Series(rfm.feature_importances_, index=list(X)).sort_values(ascending=False)
print(featimp)

# Predicting using the Random Forest model
Y_pred = rfm.predict(X_test)

# Model evaluation
print('MAE:', metrics.mean_absolute_error(Y_test, Y_pred))
print('MSE:', metrics.mean_squared_error(Y_test, Y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))
print('R2 for train:', rfm.score(X_train, Y_train))
print('R2 for test(cross validation):', rfm.score(X_test, Y_test))

"""
> Introducing additional parameters and hypertuning the model.

"""

n_estimators = [50,100,150,250,300,350,400,450] # number of trees in the random forest
max_features = ['auto', 'sqrt'] # number of features in consideration at every split
max_depth = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree
min_samples_split = [2, 6, 10] # minimum sample number to split a node
min_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node
bootstrap = [True, False] # method used to sample data points

random_grid = {'n_estimators': n_estimators,

'max_features': max_features,

'max_depth': max_depth,
'min_samples_split': min_samples_split,

'min_samples_leaf': min_samples_leaf,

'bootstrap': bootstrap}

rfm2 = RandomForestRegressor()

from sklearn.model_selection import RandomizedSearchCV
rf_random = RandomizedSearchCV(estimator = rfm2,param_distributions = random_grid,
               n_iter = 100, cv = 5, verbose=2,scoring='r2', random_state=35, n_jobs = -1)

# calculating different regression metrics
from sklearn.model_selection import GridSearchCV

rf_random.fit(X_train, Y_train)

print ('Random grid: ', random_grid, '\n')
# print the best parameters
print ('Best Parameters: ', rf_random.best_params_, ' \n')

best_result = rf_random.best_score_ # Mean cross-validated score of the best_estimator\n",
print("Best mean cross-validated score:", best_result)

#Using the best parameter.
randmf = RandomForestRegressor(n_estimators = 350, min_samples_split = 2, min_samples_leaf= 1, max_features = 'auto', max_depth= 70, bootstrap=True) 
randmf.fit( X_train, Y_train)

bestfeatures = pd.Series(randmf.feature_importances_, index=list(X)).sort_values(ascending=False)
print("significant features:", bestfeatures)

Y_pred2 = randmf.predict(X_test)

y_pred_rf1 = pd.DataFrame( { "actual": Y_test, 
"predicted_prob": randmf.predict( 
( X_test ) ) } ) 
y_pred_rf1

# Model evaluation
print('MAE:', metrics.mean_absolute_error(Y_test, Y_pred2))
print('MSE:', metrics.mean_squared_error(Y_test, Y_pred2))
print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred2)))
print('R2 for train:', randmf.score(X_train, Y_train))
print('R2 for test(cross validation):', randmf.score(X_test, Y_test))

"""## Metrics for baseline model
MAE: 182.61629455750113\
MSE: 78543.29427104407\
RMSE: 280.2557658123095\
R2 for train: 0.9983963667839264\
R2 for test(cross validation): 0.9904058584608337

---------------------------------------------------------------------------
## Metrics for tuned model
MAE: 30.341880319112803\
MSE: 2700.7856865467725\
RMSE: 51.96908394946723\
R2 for train: 0.9999547543509255\
R2 for test(cross validation): 0.9996700963413342


---------------------------------------------------------------------------

The RMSE for tuned model has low score with the R square for train and test sets shows an optimal fit.

# Gradient Boost Regressor

Gradient boosting gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees.
"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as mse

# Print to check out the size of the train and test datasets
print(f'X_train: {X_train.shape}')
print(f'X_test:  {X_test.shape}')
print(f'y_train: {Y_train.shape}')
print(f'y_test:  {Y_test.shape}')

gbm = GradientBoostingRegressor(random_state=1) # could include these parameters max_depth=2, n_estimators=3, learning_rate=1.0
gbm.fit(X_train,Y_train)

# Predicting using the Gradient Boosting Model
gbm_pred = gbm.predict(X_test)

# Model evaluation
from sklearn import metrics
from sklearn.metrics import r2_score

print('MAE:', metrics.mean_absolute_error(Y_test, gbm_pred))
print('MSE:', metrics.mean_squared_error(Y_test, gbm_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, gbm_pred)))
print('R2 for Train', gbm.score( X_train, Y_train ))
print('R2 for Test (cross validation)', gbm.score(X_test, Y_test))

## Displaying important features
# importance features
featimp_gbm = pd.Series(gbm.feature_importances_, index=list(X)).sort_values(ascending=False)
print(featimp_gbm)

# Plot feature importances
featimp_plot_gbm = pd.Series(gbm.feature_importances_, X.columns)
n = 10
plt.figure(figsize=(10,n/2))
plt.title(f'Top {n} features')
featimp_plot_gbm.sort_values()[-n:].plot.barh(color='grey');

"""### Hyperparameter tuning"""

# Hyper parameters with streamlined range for tuning 
# tuning withreduced parameters
parameters={"max_depth" : [3,5,10],
            "min_samples_split" : [5,10,15],
           "min_samples_leaf":[3,5,10],
           "learning_rate":[0.1,0.3,0.5,1],
           "max_leaf_nodes":[10,20,30],
           "n_estimators": [10,20,30,50,100,150,200,250]}

# Using grid search to select th best hyperparameters
from sklearn.model_selection import GridSearchCV
tuning_model_gbm=GridSearchCV(gbm,param_grid=parameters,scoring='r2',cv=5) # with cross-validation = 5

# tuning the model
tuning_model_gbm.fit(X_train,Y_train)
# best hyperparameters 
tuning_model_gbm.best_params_

# tuned model with best hyperparameters
gbm_tuned= GradientBoostingRegressor(learning_rate = 0.1, max_depth=10, max_leaf_nodes=30, min_samples_leaf=10,min_samples_split=5,n_estimators=250)
gbm_tuned.fit(X_train,Y_train)

# Predicting using the tuned Gradient boosted model
gbm_pred_tuned = gbm_tuned.predict(X_test)

gbm_pred_tuned = pd.DataFrame( { "actual": Y_test, 
"predicted_prob": gbm_tuned.predict( 
( X_test ) ) } ) 
gbm_pred_tuned

# Model evaluation for the tuned model
from sklearn import metrics
from sklearn.metrics import r2_score

print('MAE:', metrics.mean_absolute_error(Y_test, gbm_pred_tuned))
print('MSE:', metrics.mean_squared_error(Y_test, gbm_pred_tuned))
print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, gbm_pred_tuned)))
print('R2 for Train', gbm_tuned.score( X_train, Y_train ))
print('R2 for Test (cross validation)', gbm_tuned.score(X_test, Y_test))

## Displaying important features
# importance features
featimp_gbm_tuned = pd.Series(gbm_tuned.feature_importances_, index=list(X)).sort_values(ascending=False)
print(featimp_gbm_tuned)

"""## Evaluation Metrics for baseline model
MAE: 55.238654975427245\
MSE: 5985.058129745973\
RMSE: 77.36315744426395\
R2 for Train 0.9994641794455871\
R2 for Test (cross validation) 0.9992689191948231

-------------------------------------------------------
## Evaluation Metrics for tuned model

MAE: 31.204537903740636\
MSE: 2264.6052282208534\
RMSE: 47.58786849839834\
R2 for Train 0.9999645820428581\
R2 for Test (cross validation) 0.9997233762182814


---
The RMSE is lower with the hyperparameter tuned model. Hence its considered the best model.

# Support Vector Regression

The objective of a support vector machine algorithm is to find a hyperplane in an n-dimensional space that distinctly classifies the data points. The data points on either side of the hyperplane that are closest to the hyperplane are called Support Vectors. These influence the position and orientation of the hyperplane and thus help build the SVM.
"""

from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as mse

# Print to check out the size of the train and test datasets
print(f'X_train: {X_train.shape}')
print(f'X_test:  {X_test.shape}')
print(f'y_train: {Y_train.shape}')
print(f'y_test:  {Y_test.shape}')

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.values)
X_test_scaled = scaler.fit_transform(X_test.values)

svm = SVR() # could include these parameters max_depth=2, n_estimators=3, learning_rate=1.0
svm.fit(X_train_scaled,Y_train)

# Prediction
svm_pred = svm.predict(X_test_scaled)

# Model evaluation
from sklearn import metrics
from sklearn.metrics import r2_score

print('MAE:', metrics.mean_absolute_error(Y_test, svm_pred))
print('MSE:', metrics.mean_squared_error(Y_test, svm_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, svm_pred)))
print('R2 for Train', svm.score( X_train_scaled, Y_train ))
print('R2 for Test (cross validation)', svm.score(X_test_scaled, Y_test))

""" The important parameters having higher impact on model performance, “kernel”, “gamma” and “C”.
A Kenel function transform the feature space to a relatively higherdimensional space (create more variables out of existing variables).
 Higher value of gamma will mean that radius of influence is limited to only support vectors. This would essentially mean that the model tries and overfit.
 The value of C controls the number of slack data points.
"""

# Hyperparameter tuning
from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score
from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval

from sklearn.model_selection import GridSearchCV
  
# defining parameter range
param_grid = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']} 
  
grid = GridSearchCV(svm, param_grid, refit = True, verbose = 3)
  
# fitting the model for grid search
grid.fit(X_train_scaled, Y_train)

# print best parameter after tuning
print(grid.best_params_)
  
# print how our model looks after hyper-parameter tuning
print(grid.best_estimator_)

"""The larger the value of C, the smaller the sum of allowed slack distances. A very large value of C
will create a model without any slack data points. A support vector model with higher c value is unregularised model.
"""

svm_pred2 = grid.predict(X_test_scaled)
svm_pred2 = pd.DataFrame( { "actual": Y_test, 
"predicted_prob": grid.predict( 
( X_test_scaled ) ) } ) 
svm_pred2

print('MAE:', metrics.mean_absolute_error(Y_test, svm_pred2))
print('MSE:', metrics.mean_squared_error(Y_test, svm_pred2))
print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, svm_pred2)))
print('R2 for Train', svm.score( X_train_scaled, Y_train ))
print('R2 for Test (cross validation)', svm.score(X_test_scaled, Y_test))

"""## Evaluation Metrics for baseline model

MAE: 2196.13531435848\
MSE: 8516949.58110781\
RMSE: 2918.3813289403784\
R2 for Train -0.017497359184419414\
R2 for Test (cross validation) -0.0403538649793469

---------------------------------------------------------------------------
## Evaluation Metrics for tuned model

MAE: 111.34425708733369\
MSE: 26035.004412884657\
RMSE: 161.35366253322127\
R2 for Train -0.017497359184419414\
R2 for Test (cross validation) -0.0403538649793469

---------------------------------------------------------------------------

With the Support Vector model the R-squared values show its a bad fit. Hence disregarding the RMSE scores for the model.

##Comparing the differet Regression models like Linear Regression, Decision Tree, Random Forest, Gradient boosting and Support vector models, we choose the model with lowest RMSE score for deployment.
##The best model for deployment is the **GRADIENT BOOST REGRESSOR** with the tuned hyperparameters.
MAE: 31.204537903740636\
MSE: 2264.6052282208534\
RMSE: 47.58786849839834\
R2 for Train 0.9999645820428581\
R2 for Test (cross validation) 0.9997233762182814
With the RMSE score of 47.5 it has the lowest score of all the models in comparision and the R-squared for train and test set also show the model is a great fit.
"""