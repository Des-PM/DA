# -*- coding: utf-8 -*-
"""CA_Decisiontree_hypertuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15seipObv1gRH6xHcmwTdWl9NyoXke_Ms
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE  # imblearn library can be installed using pip install imblearn
from sklearn.preprocessing import StandardScaler
from imblearn.pipeline import Pipeline
from sklearn import metrics
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn.datasets as dataz
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as MSE
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Importing dataset and examining it
df = pd.read_csv("/content/telecom_customer_churn.csv")
pd.set_option('display.max_columns',None) #Will ensure that all columns are displayed

#Display the top 5 rows
df.head()
#Display the bottom 5 rows
#dataset.tail()

df.shape
df.describe() # Quick Description of the data

df.describe(include='all') # will include all variables

df=df.drop(columns=['Zip Code','Latitude','Longitude','Customer ID','Churn Category','Churn Category','City'])

df.info()  # general information about the dataset, including data type and missing values.

#Using Seaborn library 
sns.pairplot(df)

sns.heatmap(df.corr(), annot= True)

# Increase the size of the heatmap.
plt.figure(figsize=(16, 6))
# Store heatmap object in a variable to easily access it when you want to include more features (such as title).
# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.
heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)
# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);

plt.figure(figsize=(16, 6))
heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);
# save heatmap as .png file
# dpi - sets the resolution of the saved image in dots/inches
# bbox_inches - when set to 'tight' - does not allow the labels to be cropped
plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')

#Heatmap with cluster information
sns.clustermap(df.corr(), method="complete",cmap='RdBu',annot=True,annot_kws={"size":10}, vmin=1,figsize=(10,7))

"""In this dataset, 'Total Revenue' is the target variable. dabl helps to understand how the distribution of the total revenue looks like and also what kind of relationship different features have with total revenue. The relationship of continuous features with the target is seen in scatter plot. From the first scatter plot it's clear that total charges have strong relationship with total revenue similarly tenure in months also has a strong relation with total revenue whereas total refunds has a weak relation with total revenue. The categorical features of the dataset are visualized using a box plot where the data is distributed based on the median and the quartile range and it also shows the outliers."""

#use dabl to explore the relationships
!pip install dabl
import dabl
dabl.plot(df, 'Total Revenue')

df = pd.get_dummies(df, prefix = None)
df.head()

df.info()

df['Avg Monthly Long Distance Charges'].fillna(df['Avg Monthly Long Distance Charges'].mean(),inplace = True)
df['Avg Monthly GB Download'].fillna(df['Avg Monthly GB Download'].mean(),inplace = True)

X=df.drop(columns=['Total Revenue'])
Y=df.loc[:,'Total Revenue']

X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size =0.3, random_state = 7)

# Print to check out the size of the train and test datasets
print(f'X_train: {X_train.shape}')
print(f'X_test:  {X_test.shape}')
print(f'y_train: {Y_train.shape}')
print(f'y_test:  {Y_test.shape}')

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.values)
X_test_scaled = scaler.fit_transform(X_test.values)

"""#Decision Tree

A decision tree has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes. a decision tree starts with a root node, which does not have any incoming branches. The outgoing branches from the root node then feed into the internal nodes, also known as decision nodes. Based on the available features, both node types conduct evaluations to form homogenous subsets, which are denoted by leaf nodes, or terminal nodes. The leaf nodes represent all the possible outcomes within the dataset.
"""

# Train the decision tree regression
tm = DecisionTreeRegressor(criterion='squared_error', max_depth=3)
#fit with scaled variables
tm.fit(X_train, Y_train)
y_pred=tm.predict(X_test)

print('MAE:', metrics.mean_absolute_error(Y_test, y_pred))
print('MSE:', metrics.mean_squared_error(Y_test, y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred)))
print('R2 for train:', tm.score(X_train, Y_train))
print('R2 for test(cross validation):', tm.score(X_test, Y_test))

#Display the tree
import graphviz
from sklearn.tree import export_graphviz
       
dot_data = export_graphviz(tm, 
        out_file=None,
        max_depth=2,
        feature_names=X.columns, 
        #class_names=tm.score, 
        impurity=False, 
        filled=True, 
        #proportion=True, 
        #rotate=True,  
        rounded=True)
        
graphviz.Source(dot_data)

# Decision Tree v2
# Train the Decision Tree regression
tm2 = DecisionTreeRegressor(criterion='squared_error',
                              #max_depth=5, #maximum depth of the tree
                              min_samples_leaf=0.01,  #imposes the condition such that each leaf at least contains 10% of the training dataset\n",
                              random_state=3,
  )
        
        # fit with the scaled variable
tm2.fit(X_train, Y_train)
Y_pred2 = tm2.predict(X_test)

print('MAE:', metrics.mean_absolute_error(Y_test, Y_pred2))
print('MSE:', metrics.mean_squared_error(Y_test, Y_pred2))
print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred2)))
print('R2 for train:', tm2.score(X_train, Y_train))
print('R2 for test(cross validation):', tm2.score(X_test, Y_test))

# Checking predicted and labelled with a scatter plot
plt.scatter(Y_test,y_pred) #tree1

# Checking predicted and labelled with a scatter plot
plt.scatter(Y_test,Y_pred2) #tree2

"""Hyper Parameter Tuning."""

# Hyper parameters range intialization for tuning 
# tuning withreduced parameters
parameters={"splitter":["best","random"],
              "max_depth" : [3,5,10],
               "min_samples_split" : [5,10,15],
               "min_samples_leaf":[3,5,10],
               "min_weight_fraction_leaf":[0.1,0.3,0.5],
               "max_features":["auto","log2","sqrt",None],
               "max_leaf_nodes":[10,20,30] }

"""

*    **max_depth** indicates how deep the tree can be. The deeper the tree, the more splits it has and it captures more information about the data.
*   **min_samples_split** represents the minimum number of samples required to split an internal node. This can vary between considering at least one sample at each node to considering all of the samples at each node. When we increase this parameter, the tree becomes more constrained as it has to consider more samples at each node.

*  **min_samples_leaf** is The minimum number of samples required to be at a leaf node. This parameter is similar to min_samples_splits, however, this describe the minimum number of samples of samples at the leafs, the base of the tree.
*   **max_features** represents the number of features to consider when looking for the best split.


"""

# calculating different regression metrics
from sklearn.model_selection import GridSearchCV

tuning_model=GridSearchCV(tm,param_grid=parameters,scoring='neg_mean_squared_error',cv=5,verbose=3) # with cross-validation = 5

# function for calculating how much time take for hyperparameter tuning
def timer(start_time=None):
   if not start_time:
      start_time=datetime.now()
      return start_time
   elif start_time:
       thour,temp_sec=divmod((datetime.now()-start_time).total_seconds(),3600)
       tmin,tsec=divmod(temp_sec,60)
       print(thour,":",tmin,':',round(tsec,2))

# Commented out IPython magic to ensure Python compatibility.
# #Tuning the model
# %%capture
# from datetime import datetime
#         
# start_time=timer(None)
#         
# tuning_model.fit(X,Y)
#       
# timer(start_time)

# best hyperparameters
tuning_model.best_params_

"""Using the best parameters to fit the model."""

# tuning the model
tm_tuned_hyper= DecisionTreeRegressor(max_depth=5,max_features='auto', max_leaf_nodes=30, min_samples_leaf=3,min_samples_split=10,min_weight_fraction_leaf=0.1,splitter='best')

#fitting the model
tm_tuned_hyper.fit(X_train,Y_train)

# importance features
featimp = pd.Series(tm_tuned_hyper.feature_importances_, index=list(X)).sort_values(ascending=False)
print(featimp)

# prediction
y_pred_tuned=tm_tuned_hyper.predict(X_test)

y_pred_rf1 = pd.DataFrame( { "actual": Y_test, 
"predicted_prob": tm_tuned_hyper.predict( 
( X_test ) ) } ) 
y_pred_rf1

# Model evaluation
print('MAE:', metrics.mean_absolute_error(Y_test, y_pred_tuned))
print('MSE:', metrics.mean_squared_error(Y_test, y_pred_tuned))
print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred_tuned)))
print('R2 for train:', tm_tuned_hyper.score(X_train, Y_train))
print('R2 for test(cross validation):', tm_tuned_hyper.score(X_test, Y_test))

plt.scatter(Y_test,y_pred_tuned)

"""## Evaluation Metrics for baseline model
MAE: 505.5746172629663\
MSE: 466608.416231677\
RMSE: 683.0874147806245\
R2 for train: 0.9472779270931283\
R2 for test(cross validation): 0.9430033177212521


---
## Evaluation Metrics for tuned model
MAE: 557.6000537875832\
MSE: 641430.1928378067\
RMSE: 800.8933717030043\
R2 for train: 0.9222076755082873\
R2 for test(cross validation): 0.9216486637758795


---
The RMSE score for the baseline model is lower compared to the tuned model. Hence it is best to use the baseline model for prediction.



"""